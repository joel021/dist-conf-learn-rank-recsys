{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-09T18:07:12.079589852Z",
     "start_time": "2026-01-09T18:07:11.858690391Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from scipy import stats\n",
    "\n",
    "from recsysconfident.utils.files import sort_paths_by_datetime\n",
    "\n",
    "def find_subfolders_with_prefix(root_folder: str, prefix: str):\n",
    "\n",
    "  subfolders = []\n",
    "  for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "    for dirname in dirnames:\n",
    "      if dirname.startswith(prefix):\n",
    "        subfolders.append(os.path.join(dirpath, dirname))\n",
    "  return subfolders\n",
    "\n",
    "def read_json(path: str) -> dict:\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def generate_latex_table_from_dataframe(df: pd.DataFrame, caption: str, label: str, columns: list):\n",
    "\n",
    "    df = df[~df.index.str.contains(\"std\")]\n",
    "    std_columns = []\n",
    "    columns1 = []\n",
    "    for col in list(df.columns):\n",
    "        if \"std\" in col:\n",
    "            std_columns.append(col)\n",
    "        else:\n",
    "            columns1.append(col)\n",
    "\n",
    "    if not columns:\n",
    "        columns = columns1\n",
    "\n",
    "    for col in columns + std_columns:\n",
    "        df.loc[:, col] = df[col].astype(float).round(4)\n",
    "\n",
    "    df_bolded = df.astype(str)\n",
    "    for idx, row in df[columns].iterrows():\n",
    "\n",
    "        bold_value = row.max()\n",
    "\n",
    "        for col in columns:\n",
    "            if row[col] == bold_value:\n",
    "                df_bolded.at[idx, col] = \"\\\\textbf{\"+str(row[col])+\"}\"\n",
    "\n",
    "    for std_col in std_columns:\n",
    "        col_name = std_col[:-4]\n",
    "        formatted_col = df_bolded[col_name].astype(str) + \" $ \\\\pm $ \" + df_bolded[std_col].astype(str)\n",
    "        df_bolded[col_name] = formatted_col\n",
    "\n",
    "    df_bolded = df_bolded[columns]\n",
    "    df_bolded = df_bolded.reset_index().rename(columns={'index': 'metric'})\n",
    "\n",
    "    latex_code = df_bolded.to_latex(\n",
    "        label=label,\n",
    "        caption=caption,\n",
    "        index=False,\n",
    "        escape=False,  # Prevent escaping special characters\n",
    "        column_format=\"c\" * len(columns)  # Center align columns\n",
    "    )\n",
    "    return latex_code\n",
    "\n",
    "def get_models_metrics(dataset_uris) -> dict:\n",
    "\n",
    "    models_metrics_dfs = {}\n",
    "    for path in dataset_uris:\n",
    "        if \"data_splits\" in path:\n",
    "            continue\n",
    "\n",
    "        setup = read_json(sort_paths_by_datetime(glob.glob(f\"{path}/setup-*.json\"))[-1])\n",
    "        model_name = setup['model_name']\n",
    "\n",
    "        metrics_list = sort_paths_by_datetime(glob.glob(f\"{path}/metrics-*.json\"))\n",
    "        metrics_df = pd.DataFrame.from_dict([read_json(metrics_list[-1])[split_name]])\n",
    "\n",
    "        if model_name in models_metrics_dfs:\n",
    "            models_metrics_dfs[model_name] = pd.concat([models_metrics_dfs[model_name], metrics_df], axis=0)\n",
    "        else:\n",
    "            models_metrics_dfs[model_name] = metrics_df\n",
    "    return models_metrics_dfs\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T18:07:12.263769751Z",
     "start_time": "2026-01-09T18:07:12.095385051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "group_name = \"\"\n",
    "model_names = ['dgat', 'mf', 'mf-cluster', 'att']\n",
    "metrics = [\"mNDCG@10\", \"mAP@10\", \"mRecall@10\", \"MRR@10\", \"mNDCG@3\", \"mAP@3\", \"mRecall@3\", \"MRR@3\"]\n",
    "\n",
    "split_name = \"test\"\n",
    "\n",
    "datasets_uris = {\n",
    "\"amazon-beauty\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"amazon-beauty\"),\n",
    "  \"amazon-movies-tvs\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"amazon-movies-tvs\"),\n",
    "  \"jester-joke\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"jester-joke\"),\n",
    "  \"ml-1m\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"ml-1m\"),\n",
    "}"
   ],
   "id": "bb8439244cb6111f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T18:07:12.897818770Z",
     "start_time": "2026-01-09T18:07:12.270065709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics_ds = {}\n",
    "\n",
    "for dataset_name in datasets_uris.keys():\n",
    "\n",
    "    models_metrics_dfs_dict = get_models_metrics(datasets_uris[dataset_name])\n",
    "\n",
    "    metrics_model_rank = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "\n",
    "        mean_metrics_df = models_metrics_dfs_dict[model_name].astype(float).mean()\n",
    "        mean_metrics_df = mean_metrics_df.to_frame(name=model_name) #index: metrics names, columns: [mean]\n",
    "\n",
    "        std_metrics_df = models_metrics_dfs_dict[model_name].astype(float).std()\n",
    "        std_metrics_df = std_metrics_df.to_frame(name=f'{model_name}_std')\n",
    "\n",
    "        if dataset_name in metrics_ds:\n",
    "            metrics_ds[dataset_name] = pd.concat([metrics_ds[dataset_name], mean_metrics_df, std_metrics_df], axis=1)\n",
    "        else:\n",
    "            metrics_ds[dataset_name] = pd.concat([mean_metrics_df, std_metrics_df], axis=1)\n",
    "\n",
    "        for metric in metrics:\n",
    "\n",
    "            if not (metric in metrics_model_rank.keys()):\n",
    "                metrics_model_rank[metric] = [{\"model_name\": model_name, \"value\": float(mean_metrics_df.loc[metric].values[0])}]\n",
    "            else:\n",
    "                metrics_model_rank[metric].append({\"model_name\": model_name, \"value\": float(mean_metrics_df.loc[metric].values[0])})\n"
   ],
   "id": "ff30a406fd093174",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T18:07:13.223802574Z",
     "start_time": "2026-01-09T18:07:12.910088559Z"
    }
   },
   "cell_type": "code",
   "source": "model_names",
   "id": "afacecdf71a2a62f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dgat', 'mf', 'mf-cluster', 'att']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T18:07:14.373739839Z",
     "start_time": "2026-01-09T18:07:13.865429866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(generate_latex_table_from_dataframe(metrics_ds['amazon-movies-tvs'],\n",
    "                                          'Models performance over test split of amazon-beauty dataset.',\n",
    "                                          \"tab:amazon-movies-tvs\", model_names))\n",
    "\n",
    "print(generate_latex_table_from_dataframe(metrics_ds['ml-1m'], 'Models performance over test split of ml-1m dataset.', \"tab:ml-1m-ranking\", model_names))\n",
    "\n",
    "print(generate_latex_table_from_dataframe(metrics_ds['jester-joke'], 'Metrics of the models in test split of jester-joke.',\"tab:jester-joke-ranking\", model_names))\n",
    "\n",
    "#print(generate_latex_table_from_dataframe(metrics_ds['rotten-tomatoes'], 'Metrics of the models in test split of rotten-tomatoes.',\"tab:rotten-tomatoes-ranking\", model_names))\n"
   ],
   "id": "529b1a417795c2ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Models performance over test split of amazon-beauty dataset.}\n",
      "\\label{tab:amazon-movies-tvs}\n",
      "\\begin{tabular}{cccc}\n",
      "\\toprule\n",
      "metric & dgat & mf & mf-cluster & att \\\\\n",
      "\\midrule\n",
      "mNDCG@10 & \\textbf{0.2053} $ \\pm $ 0.2162 & 0.1477 $ \\pm $ 0.0674 & 0.1589 $ \\pm $ 0.0428 & 0.1717 $ \\pm $ 0.0154 \\\\\n",
      "mAP@10 & \\textbf{0.231} $ \\pm $ 0.2549 & 0.1503 $ \\pm $ 0.0621 & 0.1617 $ \\pm $ 0.0373 & 0.1695 $ \\pm $ 0.0083 \\\\\n",
      "mRecall@10 & 0.7604 $ \\pm $ 0.2741 & \\textbf{0.8102} $ \\pm $ 0.0263 & 0.6763 $ \\pm $ 0.3249 & 0.696 $ \\pm $ 0.2976 \\\\\n",
      "MRR@10 & 0.2235 $ \\pm $ 0.1234 & 0.2808 $ \\pm $ 0.1365 & 0.3047 $ \\pm $ 0.0862 & \\textbf{0.3236} $ \\pm $ 0.0443 \\\\\n",
      "mNDCG@3 & 0.1403 $ \\pm $ 0.1167 & 0.1426 $ \\pm $ 0.0792 & 0.1525 $ \\pm $ 0.0561 & \\textbf{0.1635} $ \\pm $ 0.0324 \\\\\n",
      "mAP@3 & 0.1533 $ \\pm $ 0.1393 & 0.1433 $ \\pm $ 0.0795 & 0.1546 $ \\pm $ 0.0527 & \\textbf{0.1659} $ \\pm $ 0.029 \\\\\n",
      "mRecall@3 & \\textbf{0.8018} $ \\pm $ 0.2398 & 0.7509 $ \\pm $ 0.157 & 0.6695 $ \\pm $ 0.3406 & 0.704 $ \\pm $ 0.2972 \\\\\n",
      "MRR@3 & 0.178 $ \\pm $ 0.1137 & 0.2219 $ \\pm $ 0.1234 & 0.2388 $ \\pm $ 0.0875 & \\textbf{0.2578} $ \\pm $ 0.0528 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\caption{Models performance over test split of ml-1m dataset.}\n",
      "\\label{tab:ml-1m-ranking}\n",
      "\\begin{tabular}{cccc}\n",
      "\\toprule\n",
      "metric & dgat & mf & mf-cluster & att \\\\\n",
      "\\midrule\n",
      "rmse & \\textbf{5.054} $ \\pm $ 0.4038 & 1.8068 $ \\pm $ 0.1698 & nan $ \\pm $ nan & nan $ \\pm $ nan \\\\\n",
      "mae & \\textbf{3.8995} $ \\pm $ 0.3212 & 1.5092 $ \\pm $ 0.1617 & nan $ \\pm $ nan & nan $ \\pm $ nan \\\\\n",
      "mNDCG@10 & \\textbf{0.9475} $ \\pm $ 0.0098 & 0.7635 $ \\pm $ 0.0919 & 0.5924 $ \\pm $ 0.2625 & 0.5927 $ \\pm $ 0.2596 \\\\\n",
      "mAP@10 & \\textbf{0.937} $ \\pm $ 0.0219 & 0.7362 $ \\pm $ 0.0828 & 0.5942 $ \\pm $ 0.2671 & 0.5865 $ \\pm $ 0.2533 \\\\\n",
      "mRecall@10 & \\textbf{0.9318} $ \\pm $ 0.0173 & 0.7362 $ \\pm $ 0.0828 & 0.6246 $ \\pm $ 0.0787 & 0.4401 $ \\pm $ 0.2628 \\\\\n",
      "MRR@10 & \\textbf{0.9998} $ \\pm $ 0.0002 & 0.9139 $ \\pm $ 0.078 & 0.7345 $ \\pm $ 0.2027 & 0.7339 $ \\pm $ 0.2049 \\\\\n",
      "mNDCG@3 & \\textbf{0.9978} $ \\pm $ 0.001 & 0.8301 $ \\pm $ 0.1143 & 0.6063 $ \\pm $ 0.2772 & 0.6091 $ \\pm $ 0.2741 \\\\\n",
      "mAP@3 & \\textbf{0.9974} $ \\pm $ 0.0014 & 0.8244 $ \\pm $ 0.1118 & 0.6077 $ \\pm $ 0.2812 & 0.6093 $ \\pm $ 0.2709 \\\\\n",
      "mRecall@3 & \\textbf{0.9973} $ \\pm $ 0.0013 & 0.8244 $ \\pm $ 0.1118 & 0.6979 $ \\pm $ 0.1404 & 0.4482 $ \\pm $ 0.2915 \\\\\n",
      "MRR@3 & \\textbf{0.9998} $ \\pm $ 0.0002 & 0.9098 $ \\pm $ 0.0856 & 0.7089 $ \\pm $ 0.225 & 0.7093 $ \\pm $ 0.227 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\caption{Metrics of the models in test split of jester-joke.}\n",
      "\\label{tab:jester-joke-ranking}\n",
      "\\begin{tabular}{cccc}\n",
      "\\toprule\n",
      "metric & dgat & mf & mf-cluster & att \\\\\n",
      "\\midrule\n",
      "rmse & \\textbf{1.6483} $ \\pm $ 0.5134 & 1.1134 $ \\pm $ 0.0095 & nan $ \\pm $ nan & nan $ \\pm $ nan \\\\\n",
      "mae & \\textbf{1.5527} $ \\pm $ 0.3968 & 1.0766 $ \\pm $ 0.0077 & nan $ \\pm $ nan & nan $ \\pm $ nan \\\\\n",
      "mNDCG@10 & 0.3494 $ \\pm $ 0.0284 & \\textbf{0.5402} $ \\pm $ 0.0039 & 0.2558 $ \\pm $ 0.1089 & 0.2427 $ \\pm $ 0.1165 \\\\\n",
      "mAP@10 & 0.3534 $ \\pm $ 0.0237 & \\textbf{0.5324} $ \\pm $ 0.0045 & 0.2638 $ \\pm $ 0.1214 & 0.2467 $ \\pm $ 0.1114 \\\\\n",
      "mRecall@10 & 0.6409 $ \\pm $ 0.0327 & 0.4689 $ \\pm $ 0.0037 & \\textbf{0.7332} $ \\pm $ 0.1281 & 0.7306 $ \\pm $ 0.1616 \\\\\n",
      "MRR@10 & 0.5353 $ \\pm $ 0.054 & \\textbf{0.7137} $ \\pm $ 0.0223 & 0.4244 $ \\pm $ 0.1093 & 0.4023 $ \\pm $ 0.1373 \\\\\n",
      "mNDCG@3 & 0.3558 $ \\pm $ 0.0595 & \\textbf{0.5621} $ \\pm $ 0.0095 & 0.2524 $ \\pm $ 0.1136 & 0.2333 $ \\pm $ 0.1317 \\\\\n",
      "mAP@3 & 0.3662 $ \\pm $ 0.0673 & \\textbf{0.5721} $ \\pm $ 0.0087 & 0.2544 $ \\pm $ 0.1146 & 0.2295 $ \\pm $ 0.1319 \\\\\n",
      "mRecall@3 & 0.6113 $ \\pm $ 0.109 & 0.4277 $ \\pm $ 0.01 & 0.7285 $ \\pm $ 0.1524 & \\textbf{0.7542} $ \\pm $ 0.1676 \\\\\n",
      "MRR@3 & 0.4887 $ \\pm $ 0.0682 & \\textbf{0.6908} $ \\pm $ 0.0287 & 0.3637 $ \\pm $ 0.1226 & 0.3381 $ \\pm $ 0.1564 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T18:07:14.499334816Z",
     "start_time": "2026-01-09T18:07:14.462675692Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d888294f30c0e71e",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
