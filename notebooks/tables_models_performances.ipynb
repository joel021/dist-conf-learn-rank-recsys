{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from scipy import stats\n",
    "\n",
    "from recsysconfident.utils.files import sort_paths_by_datetime\n",
    "\n",
    "def find_subfolders_with_prefix(root_folder: str, prefix: str):\n",
    "\n",
    "  subfolders = []\n",
    "  for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "    for dirname in dirnames:\n",
    "      if dirname.startswith(prefix):\n",
    "        subfolders.append(os.path.join(dirpath, dirname))\n",
    "  return subfolders\n",
    "\n",
    "def read_json(path: str) -> dict:\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def generate_latex_table_from_dataframe(df: pd.DataFrame, caption: str, label: str, columns: list):\n",
    "\n",
    "    df = df[~df.index.str.contains(\"std\")]\n",
    "    std_columns = []\n",
    "    columns1 = []\n",
    "    for col in list(df.model_names):\n",
    "        if \"std\" in col:\n",
    "            std_columns.append(col)\n",
    "        else:\n",
    "            columns1.append(col)\n",
    "\n",
    "    if not columns:\n",
    "        columns = columns1\n",
    "\n",
    "    for col in columns + std_columns:\n",
    "        df.loc[:, col] = df[col].astype(float).round(4)\n",
    "\n",
    "    df_bolded = df.astype(str)\n",
    "    for idx, row in df[columns].iterrows():\n",
    "\n",
    "        bold_value = row.max()\n",
    "\n",
    "        for col in columns:\n",
    "            if row[col] == bold_value:\n",
    "                df_bolded.at[idx, col] = \"\\\\textbf{\"+str(row[col])+\"}\"\n",
    "\n",
    "    for std_col in std_columns:\n",
    "        col_name = std_col[:-4]\n",
    "        formatted_col = df_bolded[col_name].astype(str) + \" $ \\\\pm $ \" + df_bolded[std_col].astype(str)\n",
    "        df_bolded[col_name] = formatted_col\n",
    "\n",
    "    df_bolded = df_bolded[columns]\n",
    "    df_bolded = df_bolded.reset_index().rename(columns={'index': 'metric'})\n",
    "\n",
    "    latex_code = df_bolded.to_latex(\n",
    "        label=label,\n",
    "        caption=caption,\n",
    "        index=False,\n",
    "        escape=False,  # Prevent escaping special characters\n",
    "        column_format=\"c\" * len(columns)  # Center align columns\n",
    "    )\n",
    "    return latex_code\n",
    "\n",
    "def get_models_metrics(dataset_uris) -> dict:\n",
    "\n",
    "    models_metrics_dfs = {}\n",
    "    for path in dataset_uris:\n",
    "        if \"data_splits\" in path:\n",
    "            continue\n",
    "\n",
    "        setup = read_json(sort_paths_by_datetime(glob.glob(f\"{path}/setup-*.json\"))[-1])\n",
    "        model_name = setup['model_name']\n",
    "\n",
    "        metrics_list = sort_paths_by_datetime(glob.glob(f\"{path}/metrics-*.json\"))\n",
    "        metrics_df = pd.DataFrame.from_dict([read_json(metrics_list[-1])[split_name]])\n",
    "\n",
    "        if model_name in models_metrics_dfs:\n",
    "            models_metrics_dfs[model_name] = pd.concat([models_metrics_dfs[model_name], metrics_df], axis=0)\n",
    "        else:\n",
    "            models_metrics_dfs[model_name] = metrics_df\n",
    "    return models_metrics_dfs\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "group_name = \"learn-rank\"\n",
    "model_names = ['uagat', 'uamf', 'dgat', 'mf']\n",
    "metrics = [\"mNDCG@10\", \"mAP@10\", \"mRecall@10\", \"MRR@10\", \"mNDCG@3\", \"mAP@3\", \"mRecall@3\", \"MRR@3\"]\n",
    "\n",
    "split_name = \"test\"\n",
    "\n",
    "datasets_uris = {\n",
    "  \"amazon-beauty\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"amazon-beauty\"),\n",
    "  \"jester-joke\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"jester-joke\"),\n",
    "  \"ml-1m\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"ml-1m\"),\n",
    "    \"rotten-tomatoes\": find_subfolders_with_prefix(f\"../runs/{group_name}/\", \"rotten-tomatoes\")\n",
    "}"
   ],
   "id": "bb8439244cb6111f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_ds = {}\n",
    "\n",
    "for dataset_name in datasets_uris.keys():\n",
    "\n",
    "    models_metrics_dfs_dict = get_models_metrics(datasets_uris[dataset_name])\n",
    "\n",
    "    metrics_model_rank = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "\n",
    "        mean_metrics_df = models_metrics_dfs_dict[model_name].astype(float).mean()\n",
    "        mean_metrics_df = mean_metrics_df.to_frame(name=model_name) #index: metrics names, columns: [mean]\n",
    "\n",
    "        std_metrics_df = models_metrics_dfs_dict[model_name].astype(float).std()\n",
    "        std_metrics_df = std_metrics_df.to_frame(name=f'{model_name}_std')\n",
    "\n",
    "        if dataset_name in metrics_ds:\n",
    "            metrics_ds[dataset_name] = pd.concat([metrics_ds[dataset_name], mean_metrics_df, std_metrics_df], axis=1)\n",
    "        else:\n",
    "            metrics_ds[dataset_name] = pd.concat([mean_metrics_df, std_metrics_df], axis=1)\n",
    "\n",
    "        for metric in metrics:\n",
    "\n",
    "            if not (metric in metrics_model_rank.keys()):\n",
    "                metrics_model_rank[metric] = [{\"model_name\": model_name, \"value\": float(mean_metrics_df.loc[metric].values[0])}]\n",
    "            else:\n",
    "                metrics_model_rank[metric].append({\"model_name\": model_name, \"value\": float(mean_metrics_df.loc[metric].values[0])})\n",
    "\n",
    "    metric_significances = []\n",
    "\n",
    "    for metric in metrics:\n",
    "        metrics_model_rank[metric] = sorted(metrics_model_rank[metric], key=lambda x: x[\"value\"], reverse=True)\n",
    "\n",
    "        model1, model2 = metrics_model_rank[metric][0]['model_name'], metrics_model_rank[metric][1]['model_name']\n",
    "        m1_df = models_metrics_dfs_dict[model1].astype(float)\n",
    "        m2_df = models_metrics_dfs_dict[model2].astype(float)\n",
    "\n",
    "        t_stat, p_val = stats.ttest_ind(m1_df[metric], m2_df[metric], equal_var=False)\n",
    "        metric_significances.append(f\"{p_val < 0.05}\")\n",
    "\n",
    "    metrics_ds[dataset_name] = pd.concat([metrics_ds[dataset_name].reset_index(drop=True), pd.DataFrame({\"p < 0.05\": metric_significances})], axis=1)\n",
    "\n",
    "    print(metrics_ds[dataset_name])"
   ],
   "id": "ff30a406fd093174",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metrics_ds['amazon-beauty']",
   "id": "90a3fe9254af666a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "print(generate_latex_table_from_dataframe(metrics_ds['amazon-beauty'],\n",
    "                                          'Models performance over test split of amazon-beauty dataset.',\n",
    "                                          \"tab:amazon-beauty-ranking\", model_names))\n",
    "\n",
    "print(generate_latex_table_from_dataframe(metrics_ds['ml-1m'], 'Models performance over test split of ml-1m dataset.', \"tab:ml-1m-ranking\", model_names))\n",
    "\n",
    "print(generate_latex_table_from_dataframe(metrics_ds['jester-joke'], 'Metrics of the models in test split of jester-joke.',\"tab:jester-joke-ranking\", model_names))\n",
    "\n",
    "print(generate_latex_table_from_dataframe(metrics_ds['rotten-tomatoes'], 'Metrics of the models in test split of rotten-tomatoes.',\"tab:rotten-tomatoes-ranking\", model_names))\n"
   ],
   "id": "529b1a417795c2ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d888294f30c0e71e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
